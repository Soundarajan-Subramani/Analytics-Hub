{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81dc9982-30cc-4123-aae0-0fe4e90a95b3",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7382fc8-ec7a-45bd-9f0e-a1440cd73e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from dateutil import parser\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59f31d-cc54-493c-b284-420c6607561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the api_key from the text file\n",
    "\n",
    "file_dir = r\"C:\\Users\\External Boot\\Downloads\\Documents\\supporting_files\\api_key.txt\"\n",
    "with open(file_dir, \"r\") as f:\n",
    "    API_KEY = f.read().strip()\n",
    "\n",
    "print(\"API Key Loaded:\", API_KEY[:8] + \"******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae471b-d77f-405f-9c83-74bc9e1cd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the api service name and api version in the builder\n",
    "youtube = build('youtube','v3', developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30978687-b16d-4f15-9b1e-ec00bc22bdd0",
   "metadata": {},
   "source": [
    "#### Getting The YouTube Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420f982-995d-4131-8d3f-2e7f3cd54e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a query list to search for the relevant channels\n",
    "queries = [\n",
    "    \"data science\",\n",
    "    \"AI\",\n",
    "    \"artificial intelligence\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"generative AI\",\n",
    "    \"LLM\",\n",
    "    \"neural networks\",\n",
    "    \"MLOps\",\n",
    "    \"data engineering\",\n",
    "    \n",
    "    # additional fields\n",
    "    \"computer vision\",\n",
    "    \"natural language processing\",\n",
    "    \"reinforcement learning\",\n",
    "    \"big data\",\n",
    "    \"cloud computing\",\n",
    "    \"edge AI\",\n",
    "    \"AI ethics\",\n",
    "    \"data visualization\",\n",
    "    \"predictive analytics\",\n",
    "    \"data mining\",\n",
    "    \"robotics\",\n",
    "    \"automation\",\n",
    "    \"cybersecurity AI\",\n",
    "    \"quantum computing\",\n",
    "    \"AI in healthcare\",\n",
    "    \"AI in finance\",\n",
    "    \"AI in education\",\n",
    "    \"AI governance\",\n",
    "    \"AutoML\",\n",
    "    \"feature engineering\",\n",
    "    \"data pipelines\",\n",
    "    \"AI research\",\n",
    "    \"AI startups\",\n",
    "    \"AI tools and frameworks\"\n",
    "]\n",
    "\n",
    "# Updated queries\n",
    "queries_updt = [x.strip().lower() for x in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c18b1-d86d-4e56-ba5b-c19c5053bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined function to get the list of channels and the respective ids\n",
    "def get_youtube_channels(\n",
    "        # query=\"data science\",   #select your niche (Data Dcience, AI, Coding, Technology, etc)\n",
    "        queries,\n",
    "        max_per_page=50,\n",
    "        pages=50   \n",
    "):\n",
    "    channels = {}\n",
    "    next_page = None\n",
    "\n",
    "    # looping to search and get the relevant channel info\n",
    "    for query in queries:\n",
    "        print(f\"Searching for: {query}\")\n",
    "\n",
    "        next_page = None                     \n",
    "        for _ in range(pages):\n",
    "            response = youtube.search().list(\n",
    "                q=query,\n",
    "                type=\"channel\",\n",
    "                part=\"snippet\",\n",
    "                order=\"relevance\",\n",
    "                maxResults=max_per_page,\n",
    "                pageToken=next_page\n",
    "            ).execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                channel_id = item[\"snippet\"][\"channelId\"]\n",
    "                title = item[\"snippet\"][\"title\"].strip()\n",
    "                channels[channel_id] = title \n",
    "\n",
    "            next_page = response.get(\"nextPageToken\")\n",
    "            if not next_page:\n",
    "                break\n",
    "\n",
    "    # Sorting the channels alphabetically \n",
    "    sorted_channels = dict(sorted(channels.items(), key=lambda x: x[1].lower()))\n",
    "    return sorted_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae183259-69bf-4169-8bc5-bbebdc7c4203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick check on the areas of interest\n",
    "for i in queries_updt:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978686b4-852b-4643-b8bf-e1b3ac07bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with channel id and title\n",
    "req_channels = get_youtube_channels(queries_updt, pages=50)\n",
    "\n",
    "channels_df = pd.DataFrame([\n",
    "    {\"channel_title\": title, \"channel_id\": cid}\n",
    "    for cid, title in req_channels.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f2a76-4d8e-4875-969f-e5d48bd6c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on the channels\n",
    "channels_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc33e4f-b8fd-448b-8300-5898b3f78183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the new directory and filename\n",
    "output_dir = Path('C:/Users/...') #select your own path\n",
    "output_filename = 'channels.csv'\n",
    "output_filepath = output_dir/output_filename\n",
    "print(output_filepath)\n",
    "\n",
    "# Creating the directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Saving the DataFrame to CSV in the new location\n",
    "channels_df.to_csv(output_filepath, index=False)\n",
    "print(\"Channels Saved Succesfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83113fbb-ce59-498a-8cef-c84eae68a18c",
   "metadata": {},
   "source": [
    "#### Fetching Relevant Data -- Looping Channel Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c9ac1-ea19-4b48-97d1-a85da1e476e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to fetch the channel info from youtube\n",
    "def get_channel_info(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    if not response[\"items\"]:\n",
    "        return None\n",
    "\n",
    "    item = response[\"items\"][0]\n",
    "    snippet = item[\"snippet\"]\n",
    "    stats = item[\"statistics\"]\n",
    "\n",
    "    # Dates\n",
    "    start_date = snippet[\"publishedAt\"]\n",
    "    start_date_dt = parser.isoparse(start_date)\n",
    "    now = datetime.now(timezone.utc)\n",
    "    age_days = (now - start_date_dt).days\n",
    "    years_active = age_days // 365\n",
    "\n",
    "    # Stats\n",
    "    subs = int(stats.get(\"subscriberCount\", 0))\n",
    "    total_videos = int(stats.get(\"videoCount\", 0))\n",
    "    total_views = int(stats.get(\"viewCount\", 0))\n",
    "\n",
    "    # Derived metrics\n",
    "    uploads_per_week = round(total_videos / (age_days / 7), 2) if (total_videos > 0 & age_days>0) else 0\n",
    "    avg_views = round(total_views / total_videos, 2) if total_videos > 0 else 0\n",
    "    avg_likes, avg_comments = get_avg_engagement(channel_id)\n",
    "\n",
    "    # Milestone\n",
    "    reached_250k = subs >= 250000\n",
    "    time_to_250k_days = None\n",
    "    if reached_250k:\n",
    "        growth_rate = subs / age_days if age_days > 0 else 0\n",
    "        time_to_250k_days = round(250000 / growth_rate, 2) if growth_rate > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"channel_id\": channel_id,\n",
    "        \"channel_title\": snippet.get(\"title\", \"\"),\n",
    "        \"niche\": snippet.get(\"title\", \"\"),  # placeholder for classification\n",
    "        \"start_date\": start_date,\n",
    "        \"current_subscriber_count\": subs,\n",
    "        \"years_active\": years_active,\n",
    "        \"channel_age_days\": age_days,\n",
    "        \"total_videos\": total_videos,\n",
    "        \"avg_views\": avg_views,\n",
    "        \"avg_likes\": avg_likes,\n",
    "        \"avg_comments\": avg_comments,\n",
    "        \"uploads_per_week\": uploads_per_week,\n",
    "        \"reached_250k\": reached_250k,\n",
    "        \"time_to_250k_days\": time_to_250k_days,\n",
    "        \n",
    "        # Simulation inputs\n",
    "        \"growth_rate_per_day\": subs / age_days if age_days > 0 else 0,\n",
    "        \"engagement_rate\": (avg_likes + avg_comments) / avg_views if avg_views > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce7143-dd07-4562-a22b-da5ebf070ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to generate additional fields\n",
    "def get_avg_engagement(channel_id, sample_size=50):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    items = response.get(\"items\", [])\n",
    "    if not items:\n",
    "        return 0, 0\n",
    "\n",
    "    uploads_playlist_id = items[0][\"contentDetails\"][\"relatedPlaylists\"].get(\"uploads\")\n",
    "    if not uploads_playlist_id:\n",
    "        return 0, 0\n",
    "\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "    try:\n",
    "        while len(video_ids) < sample_size:\n",
    "            req = youtube.playlistItems().list(\n",
    "                part=\"contentDetails\",\n",
    "                playlistId=uploads_playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            res = req.execute()\n",
    "            for item in res.get(\"items\", []):\n",
    "                video_ids.append(item[\"contentDetails\"][\"videoId\"])\n",
    "            next_page_token = res.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching playlist for {channel_id}: {e}\")\n",
    "        return 0, 0\n",
    "\n",
    "    likes, comments, count = 0, 0, 0\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        req = youtube.videos().list(\n",
    "            part=\"statistics\",\n",
    "            id=\",\".join(video_ids[i:i+50])\n",
    "        )\n",
    "        res = req.execute()\n",
    "        for item in res.get(\"items\", []):\n",
    "            stats = item[\"statistics\"]\n",
    "            likes += int(stats.get(\"likeCount\", 0))\n",
    "            comments += int(stats.get(\"commentCount\", 0))\n",
    "            count += 1\n",
    "\n",
    "    avg_likes = round(likes / count, 2) if count > 0 else 0\n",
    "    avg_comments = round(comments / count, 2) if count > 0 else 0\n",
    "    return avg_likes, avg_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b503f-eee5-4cc5-a0b8-a3c48cae18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to build the final dataframe\n",
    "def build_channel_dataframe(channel_ids, save_path):\n",
    "    data = []\n",
    "    for idx, cid in enumerate(channel_ids, start=1):\n",
    "        print(f\"[{idx}/{len(channel_ids)}] Fetching data for channel: {cid}\")\n",
    "        try:\n",
    "            info = get_channel_info(cid)\n",
    "            if info:\n",
    "                data.append(info)\n",
    "                # Save progress after each successful fetch\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_csv(save_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {cid}: {e}\")\n",
    "            # continue to next channel without breaking\n",
    "            continue\n",
    "\n",
    "    # Final DataFrame\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b6a03-6e78-40c6-b255-4009f33ca75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "channel_ids = list(channels_df['channel_id'])\n",
    "print(\"Data fetching for {}  channels\".format(len(channel_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf4a27-7480-4e17-aa82-8421532f9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the new directory and filename\n",
    "output_dir = Path('C:/Users/...') #select your own path\n",
    "output_filename = 'channels_data_progressive.csv'\n",
    "output_filepath = output_dir/output_filename\n",
    "print(output_filepath)\n",
    "\n",
    "# Creating the directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b211b-7577-4eca-91e1-1f80a50b5875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Final dataframe creation\n",
    "df = build_channel_dataframe(channel_ids, output_filepath)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2d2b9-eea1-4834-a8bb-bae85a0f61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to CSV in the new location\n",
    "df.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cb35e-d22a-4c0e-9a76-4e23e6a9e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on the dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c49f2f-a6c6-40bb-9c5a-7df8138f8a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095249a-2048-4378-b7d5-c7743d9c0ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f79fc-735d-4938-926d-2c1908c515bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def add_start_year(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    start_year = pd.to_datetime(df[\"start_date\"], format=\"ISO8601\").dt.year\n",
    "    return df.assign(start_year=start_year)\n",
    "\n",
    "def cohort_summary(\n",
    "    df: pd.DataFrame,\n",
    "    sim_results: pd.DataFrame,\n",
    "    cohort_by: str = \"niche\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize metrics by cohort (e.g., niche, start_year, uploads_per_week bucket).\n",
    "    \"\"\"\n",
    "    merged = df.merge(sim_results, on=\"channel_id\", how=\"left\")\n",
    "\n",
    "    # Optional: bucket uploads_per_week for cohorting\n",
    "    if cohort_by == \"uploads_bucket\":\n",
    "        bins = [0, 0.5, 1, 2, 4, 10, float(\"inf\")]\n",
    "        labels = [\"<=0.5\", \"0.5-1\", \"1-2\", \"2-4\", \"4-10\", \">10\"]\n",
    "        merged[\"uploads_bucket\"] = pd.cut(merged[\"uploads_per_week\"], bins=bins, labels=labels, right=True)\n",
    "        group_col = \"uploads_bucket\"\n",
    "    else:\n",
    "        group_col = cohort_by\n",
    "\n",
    "    summary = (\n",
    "        merged.groupby(group_col)\n",
    "        .agg(\n",
    "            current_subs_median=(\"current_subscriber_count\", \"median\"),\n",
    "            growth_rate_day_median=(\"growth_rate_per_day\", \"median\"),\n",
    "            engagement_rate_median=(\"engagement_rate\", \"median\"),\n",
    "            prob_250k_median=(\"prob_250k_in_horizon\", \"median\"),\n",
    "            prob_250k_mean=(\"prob_250k_in_horizon\", \"mean\"),\n",
    "            expected_end_subs_median=(\"expected_end_subs\", \"median\"),\n",
    "            channels=(\"channel_id\", \"nunique\")\n",
    "        )\n",
    "        .sort_values(\"prob_250k_median\", ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983b6b3-377b-4126-837d-e6685420ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare\n",
    "# df = add_start_year(df)\n",
    "\n",
    "# # Run Monte Carlo\n",
    "# sim_df = monte_carlo_probability(df, horizon_days=365*3, sims=5000, variability_ratio=0.25, use_lognormal=False)\n",
    "\n",
    "# # Cohort by niche\n",
    "# niche_cohort = cohort_summary(df, sim_df, cohort_by=\"niche\")\n",
    "# print(niche_cohort.head())\n",
    "\n",
    "# # Cohort by start_year\n",
    "# year_cohort = cohort_summary(df, sim_df, cohort_by=\"start_year\")\n",
    "# print(year_cohort.head())\n",
    "\n",
    "# # Cohort by uploads intensity bucket\n",
    "# uploads_cohort = cohort_summary(df, sim_df, cohort_by=\"uploads_bucket\")\n",
    "# print(uploads_cohort.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a854d31-e95c-49a4-a435-194c88703c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare\n",
    "df_tmp = add_start_year(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d24a5-6a24-42f4-a8a3-b3fd7d699da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tmp['start_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d173a90-7215-4849-9e93-be42a2567eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['start_date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124dfda-6e41-4e66-a24f-16b53b927ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11dbcf-e5b4-4e61-9de7-849cebd208af",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now(timezone.utc)\n",
    "pd.to_datetime(now,format=\"ISO8601\").date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf5f5a-2779-48b1-a829-cd15497cf9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_tmp['age_days']= (pd.to_datetime(now,format=\"ISO8601\").date - pd.to_datetime(df[\"start_date\"], format=\"ISO8601\").dt.date).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83e32e-04a2-4d5f-9dcf-7a62a1b10dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp[\"age_days\"] = (pd.to_datetime(now) - pd.to_datetime(df_tmp[\"start_date\"], format=\"ISO8601\")).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44617f-de2e-4f9e-9001-ed5a7ff40971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24aede4-d917-4196-a43c-19dcb006ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp['uplds_per_week'] = round(df_tmp['total_videos'] / (df_tmp['age_days'] / 7), 2) if (df_tmp['total_videos'] > 0 & df_tmp['age_days']>0) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcc5f4-6e0a-4e2a-aca1-0e7df990798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp[\"uplds_per_week\"] = df_tmp.apply(\n",
    "    lambda row: round(row[\"total_videos\"] * 7 / row[\"age_days\"], 2)\n",
    "    if (row[\"total_videos\"] > 0 and row[\"age_days\"] > 0)\n",
    "    else 0,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89794c15-a46a-4a57-97ee-c6ab7e4e0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['uplds_per_week'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04d722-d06c-4618-b7ef-f74a3e3435cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['uploads_per_week'] = df_tmp['uplds_per_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d11f5a-cf61-4449-8b1d-67bd1558c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['uploads_per_week'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa50805-d392-4153-b9bb-e972a49b1ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b4d0f-0ba5-4e59-8795-989043f8a15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcba16-a427-40e3-91d2-a3d31ce2811f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dc1c2-b7e8-42be-8c29-f219b84b095a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4ad5d-aae7-4fde-8913-7c769b94f1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342d3fe-f41b-4bb0-b5d0-71a2a245f31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1016973-669c-4a30-8c28-fe5f9be9873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort by niche\n",
    "niche_cohort = cohort_summary(df, sim_df, cohort_by=\"niche\")\n",
    "print(niche_cohort.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a86382-adce-4a11-a43a-b17a4a7a6e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cohort by start_year\n",
    "year_cohort = cohort_summary(df_tmp, sim_df, cohort_by=\"start_year\")\n",
    "print(year_cohort.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff729c7d-1b67-4390-8f55-3d4ce95291ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfea10-6250-4058-9016-31f1add4b4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1802d0-0237-4b2d-ab82-4b078abec302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort by uploads intensity bucket\n",
    "uploads_cohort = cohort_summary(df_tmp, sim_df, cohort_by=\"uploads_bucket\")\n",
    "print(uploads_cohort.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00e54d-5ecd-4901-9698-b7b0593a4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploads_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b8471-51fb-4fbe-9f44-7b8ed5839d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18064642-5428-4a14-adc5-7e422de7fdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16697f09-db37-4ebb-b1ad-d963bd61e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "\n",
    "def bayesian_lambda_posterior(\n",
    "    df: pd.DataFrame,\n",
    "    prior_strength_days: int = 30,\n",
    "    variability_ratio: float = 0.25\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gamma–Poisson Bayesian update for daily growth rate λ per channel.\n",
    "    We form a prior using per-channel mean growth and variability proxy.\n",
    "\n",
    "    Prior: λ ~ Gamma(a0, b0)\n",
    "    Posterior with aggregated evidence: λ ~ Gamma(a_post, b_post)\n",
    "\n",
    "    - prior_strength_days: pseudo-count of historical days influencing the prior.\n",
    "    - variability_ratio: used to shape dispersion of prior around mean.\n",
    "    \"\"\"\n",
    "    mean_growth = df[\"growth_rate_per_day\"].to_numpy(dtype=float)\n",
    "    # Construct prior parameters:\n",
    "    # Set prior mean = mean_growth, and prior variance = (variability_ratio * mean)^2\n",
    "    # For Gamma: mean = a/b, var = a/b^2 -> choose b then a = mean * b\n",
    "    # We tie b to prior_strength_days to scale certainty.\n",
    "    # Let b0 = prior_strength_days / mean_growth (avoid zero division)\n",
    "    mean_safe = np.clip(mean_growth, 1e-6, None)\n",
    "    b0 = prior_strength_days / mean_safe\n",
    "    a0 = mean_safe * b0\n",
    "\n",
    "    # Aggregate \"observed\" evidence approximated by current age and mean gains:\n",
    "    # Effective counts: total gains ≈ mean_growth * channel_age_days\n",
    "    total_days = df[\"channel_age_days\"].to_numpy(dtype=float)\n",
    "    total_gains = mean_growth * total_days\n",
    "\n",
    "    # Posterior parameters:\n",
    "    a_post = a0 + total_gains\n",
    "    b_post = b0 + total_days\n",
    "\n",
    "    # Posterior summaries:\n",
    "    post_mean = a_post / b_post\n",
    "    post_var = a_post / (b_post ** 2)\n",
    "    post_std = np.sqrt(post_var)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"channel_id\": df[\"channel_id\"],\n",
    "        \"lambda_prior_a\": a0,\n",
    "        \"lambda_prior_b\": b0,\n",
    "        \"lambda_post_a\": a_post,\n",
    "        \"lambda_post_b\": b_post,\n",
    "        \"lambda_post_mean\": post_mean,\n",
    "        \"lambda_post_std\": post_std\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def bayesian_prob_250k(\n",
    "    df: pd.DataFrame,\n",
    "    posterior_df: pd.DataFrame,\n",
    "    horizon_days: int = 365 * 3,\n",
    "    sims: int = 5000,\n",
    "    random_state: int | None = 123\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Draw λ from posterior Gamma for each channel and simulate horizon gains.\n",
    "    Compute probability of reaching 250k and credible intervals.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    merged = df.merge(posterior_df, on=\"channel_id\", how=\"left\")\n",
    "\n",
    "    current = merged[\"current_subscriber_count\"].to_numpy(dtype=float)\n",
    "    a = merged[\"lambda_post_a\"].to_numpy(dtype=float)\n",
    "    b = merged[\"lambda_post_b\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Sample lambda per sim: shape (n_channels, sims)\n",
    "    lam_samples = rng.gamma(shape=a[:, None], scale=1.0 / b[:, None], size=(len(df), sims))\n",
    "    # Predictive expected gains ~ Poisson(λ * horizon). For computation efficiency, use Normal approx for large means or sample Poisson directly:\n",
    "    gains = rng.poisson(lam_samples * horizon_days)\n",
    "    ending = current[:, None] + gains\n",
    "    hit = ending >= 250_000\n",
    "\n",
    "    prob = hit.mean(axis=1)\n",
    "    q10 = np.quantile(ending, 0.10, axis=1)\n",
    "    q50 = np.quantile(ending, 0.50, axis=1)\n",
    "    q90 = np.quantile(ending, 0.90, axis=1)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"channel_id\": merged[\"channel_id\"],\n",
    "        \"bayes_prob_250k_in_horizon\": prob,\n",
    "        \"bayes_end_subs_q10\": q10,\n",
    "        \"bayes_end_subs_q50\": q50,\n",
    "        \"bayes_end_subs_q90\": q90,\n",
    "        \"horizon_days\": horizon_days,\n",
    "        \"sims\": sims\n",
    "    })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c81e70-25a7-4561-8100-6a288182e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior for lambda per channel\n",
    "posterior_df = bayesian_lambda_posterior(df, prior_strength_days=30, variability_ratio=0.25)\n",
    "\n",
    "# Bayesian probabilities of hitting 250k\n",
    "bayes_res = bayesian_prob_250k(df, posterior_df, horizon_days=365*3, sims=5000)\n",
    "\n",
    "# Merge everything\n",
    "sim_df = monte_carlo_probability(df, horizon_days=365*3, sims=5000)\n",
    "full_res = df[[\"channel_id\", \"niche\", \"current_subscriber_count\"]].merge(sim_df, on=\"channel_id\").merge(bayes_res, on=\"channel_id\")\n",
    "print(full_res.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1827f7-81fe-42a6-9312-f9eda7117654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a03ea-f6ba-443a-8f23-a6e02bd55356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def monte_carlo_probability(\n",
    "    df: pd.DataFrame,\n",
    "    horizon_days: int = 365 * 3,\n",
    "    sims: int = 5000,\n",
    "    variability_ratio: float = 0.25,\n",
    "    use_lognormal: bool = False,\n",
    "    random_state: int | None = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorized Monte Carlo simulation of subscriber growth.\n",
    "    Returns a DataFrame with per-channel probabilities and summary stats.\n",
    "    \n",
    "    Parameters:\n",
    "    - variability_ratio: scales std around mean daily growth (e.g., 0.25 -> 25% of mean).\n",
    "    - use_lognormal: if True, model daily gains as Lognormal; else Gaussian with floor at 0.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    channels = df[\"channel_id\"].to_numpy()\n",
    "    current = df[\"current_subscriber_count\"].to_numpy(dtype=float)\n",
    "    mean_growth = df[\"growth_rate_per_day\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Scale variability by engagement and uploads for richer dynamics\n",
    "    engagement = np.clip(df[\"engagement_rate\"].to_numpy(dtype=float), 0, None)\n",
    "    uploads = np.clip(df[\"uploads_per_week\"].to_numpy(dtype=float), 0.01, None)\n",
    "    # Normalized modifiers\n",
    "    eng_mod = 1.0 + 0.5 * (engagement / (engagement.mean() + 1e-9))\n",
    "    upl_mod = 1.0 + 0.3 * (uploads / (uploads.mean() + 1e-9))\n",
    "\n",
    "    std_growth = np.clip(mean_growth * variability_ratio * eng_mod * upl_mod, 0.0, None)\n",
    "\n",
    "    n = len(channels)\n",
    "    # Shape: (n_channels, sims, horizon_days)\n",
    "    if use_lognormal:\n",
    "        # Lognormal params from mean and std (approximation)\n",
    "        mu = np.log(np.clip(mean_growth, 1e-6, None)) - 0.5 * np.log1p((std_growth / np.clip(mean_growth, 1e-6, None)) ** 2)\n",
    "        sigma = np.sqrt(np.log1p((std_growth / np.clip(mean_growth, 1e-6, None)) ** 2))\n",
    "        daily = rng.lognormal(mean=mu[:, None, None], sigma=sigma[:, None, None], size=(n, sims, horizon_days))\n",
    "    else:\n",
    "        daily = rng.normal(loc=mean_growth[:, None, None], scale=std_growth[:, None, None], size=(n, sims, horizon_days))\n",
    "        daily = np.clip(daily, 0.0, None)\n",
    "\n",
    "    cumulative_gains = daily.sum(axis=2)  # (n, sims)\n",
    "    ending_subs = current[:, None] + cumulative_gains\n",
    "    hit_250k = ending_subs >= 250_000\n",
    "\n",
    "    prob = hit_250k.mean(axis=1)\n",
    "    # Expected days to hit 250k (median over successful paths); fallback to NaN if never hit\n",
    "    # Compute first passage time for each sim:\n",
    "    threshold = 250_000 - current[:, None]\n",
    "    cum_daily = daily.cumsum(axis=2)  # (n, sims, t)\n",
    "    reached = cum_daily >= threshold[:, :, None]  # bool\n",
    "    # For each channel, sim: first time index reaching threshold\n",
    "    first_hit_idx = reached.argmax(axis=2)  # returns 0 if never reached; need mask\n",
    "    ever_hit = reached.any(axis=2)\n",
    "    # Days to hit (1-based indexing for day count)\n",
    "    days_to_hit = np.where(ever_hit, first_hit_idx + 1, np.nan)\n",
    "    # Median across sims per channel\n",
    "    median_days_to_hit = np.nanmedian(days_to_hit, axis=1)\n",
    "\n",
    "    # Expected ending subs and quantiles\n",
    "    expected_end = ending_subs.mean(axis=1)\n",
    "    q10 = np.quantile(ending_subs, 0.10, axis=1)\n",
    "    q50 = np.quantile(ending_subs, 0.50, axis=1)\n",
    "    q90 = np.quantile(ending_subs, 0.90, axis=1)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"channel_id\": channels,\n",
    "        \"prob_250k_in_horizon\": prob,\n",
    "        \"median_days_to_250k\": median_days_to_hit,\n",
    "        \"expected_end_subs\": expected_end,\n",
    "        \"end_subs_q10\": q10,\n",
    "        \"end_subs_q50\": q50,\n",
    "        \"end_subs_q90\": q90,\n",
    "        \"horizon_days\": horizon_days,\n",
    "        \"sims\": sims\n",
    "    })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36ec8b-b3d0-4a82-803a-3f2c69136c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run Monte Carlo\n",
    "sim_df = monte_carlo_probability(df_tmp, horizon_days=365*3, sims=500, variability_ratio=0.25, use_lognormal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed675ecb-81ee-4a1c-a84b-4ebfd3dfc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67971f1a-2402-41ec-96d4-8cd236bead76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf4e85-b422-45bb-8816-dbf3e8c7e82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5059545-3761-48fd-909f-a690e30d185e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be70ddf-0ff9-4952-9249-3b5490d56f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657da3a-e272-4778-8c86-8fcfeb0d4fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f23c85-5f51-4e74-a50e-4c4c3d9f92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "def kaplan_meier_analysis(df, milestone=250_000, cohort_col=None):\n",
    "    \"\"\"\n",
    "    Perform Kaplan–Meier survival analysis on channels reaching a milestone.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with at least ['channel_id', 'channel_age_days', 'current_subscriber_count']\n",
    "    - milestone: subscriber milestone (default 250k)\n",
    "    - cohort_col: optional column name to stratify curves (e.g., 'niche', 'start_year')\n",
    "    \n",
    "    Returns:\n",
    "    - kmf objects (dict if stratified), survival plot\n",
    "    \"\"\"\n",
    "    # Event flag: has channel reached milestone?\n",
    "    df[\"event\"] = df[\"current_subscriber_count\"] >= milestone\n",
    "    df[\"duration\"] = df[\"channel_age_days\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if cohort_col and cohort_col in df.columns:\n",
    "        kmf_dict = {}\n",
    "        for cohort, group in df.groupby(cohort_col):\n",
    "            kmf = KaplanMeierFitter()\n",
    "            kmf.fit(durations=group[\"duration\"], event_observed=group[\"event\"], label=str(cohort))\n",
    "            kmf.plot_survival_function(ci_show=True)\n",
    "            kmf_dict[cohort] = kmf\n",
    "        plt.title(f\"Kaplan–Meier Survival Curves by {cohort_col}\")\n",
    "    else:\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=df[\"duration\"], event_observed=df[\"event\"], label=f\"Milestone {milestone}\")\n",
    "        kmf.plot_survival_function(ci_show=True)\n",
    "        plt.title(f\"Kaplan–Meier Survival Curve (Milestone {milestone})\")\n",
    "        kmf_dict = {\"overall\": kmf}\n",
    "\n",
    "    plt.xlabel(\"Channel Age (days)\")\n",
    "    plt.ylabel(\"Probability of NOT reaching milestone\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return kmf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8093d-ba1f-4758-abdd-a759326a0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose df is your channel DataFrame with channel_age_days and current_subscriber_count\n",
    "# Add a start_year column for cohort analysis\n",
    "df[\"start_year\"] = pd.to_datetime(df[\"start_date\"]).dt.year\n",
    "\n",
    "# Overall survival curve\n",
    "kmf_overall = kaplan_meier_analysis(df)\n",
    "\n",
    "# Stratified by niche\n",
    "kmf_by_niche = kaplan_meier_analysis(df, cohort_col=\"niche\")\n",
    "\n",
    "# Stratified by start year\n",
    "kmf_by_year = kaplan_meier_analysis(df, cohort_col=\"start_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7e59a-01b4-4e05-97e8-44848dd27567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbf837-e7c5-4915-9f1a-d969f71cb9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27385889-5e59-4fde-9b56-2caeab6d913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_results(\n",
    "    df_base: pd.DataFrame,\n",
    "    mc_df: pd.DataFrame,\n",
    "    bayes_df: pd.DataFrame,\n",
    "    cohort_df: pd.DataFrame,\n",
    "    km_df: pd.DataFrame,\n",
    "    w_mc: float = 0.35,\n",
    "    w_bayes: float = 0.35,\n",
    "    w_km: float = 0.20,\n",
    "    w_expected: float = 0.10\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine Monte Carlo, Bayesian, Cohort, and Kaplan–Meier results\n",
    "    into a single final probability score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge everything\n",
    "    merged = (\n",
    "        df_base[[\"channel_id\", \"current_subscriber_count\"]]\n",
    "        .merge(mc_df, on=\"channel_id\", how=\"left\")\n",
    "        .merge(bayes_df, on=\"channel_id\", how=\"left\")\n",
    "        .merge(km_df, on=\"channel_id\", how=\"left\")\n",
    "        .merge(cohort_df, on=\"channel_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Normalize expected_end_subs to 0–1\n",
    "    exp_min = merged[\"expected_end_subs\"].min()\n",
    "    exp_max = merged[\"expected_end_subs\"].max()\n",
    "    merged[\"expected_norm\"] = (\n",
    "        (merged[\"expected_end_subs\"] - exp_min) / (exp_max - exp_min + 1e-9)\n",
    "    )\n",
    "\n",
    "    # Cohort uplift multiplier (normalized)\n",
    "    if \"cohort_score\" in merged.columns:\n",
    "        cmin = merged[\"cohort_score\"].min()\n",
    "        cmax = merged[\"cohort_score\"].max()\n",
    "        merged[\"cohort_norm\"] = (\n",
    "            (merged[\"cohort_score\"] - cmin) / (cmax - cmin + 1e-9)\n",
    "        )\n",
    "    else:\n",
    "        merged[\"cohort_norm\"] = 1.0\n",
    "\n",
    "    # Kaplan–Meier probability of reaching 250K\n",
    "    merged[\"km_prob_250k\"] = 1 - merged[\"km_survival_prob\"]\n",
    "\n",
    "    # Final probability score (geometric weighted mean)\n",
    "    merged[\"final_probability_score\"] = (\n",
    "        (merged[\"prob_250k_in_horizon\"] ** w_mc) *\n",
    "        (merged[\"bayes_prob_250k_in_horizon\"] ** w_bayes) *\n",
    "        (merged[\"km_prob_250k\"] ** w_km) *\n",
    "        (merged[\"expected_norm\"] ** w_expected) *\n",
    "        (merged[\"cohort_norm\"] ** 0.10)  # small uplift from cohort\n",
    "    )\n",
    "\n",
    "    merged[\"final_probability_score\"] = merged[\"final_probability_score\"].clip(0, 1)\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2edc7-1bfb-480e-9276-ed235b49c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = combine_all_results(\n",
    "    df_base=df,\n",
    "    mc_df=mc_results,\n",
    "    bayes_df=bayes_results,\n",
    "    cohort_df=cohort_results,     \n",
    "    km_df=km_results              \n",
    ")\n",
    "\n",
    "final_df.sort_values(\"final_probability_score\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2189f-157f-4694-8191-371153e7e70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f6fd5-97a3-4310-824c-cee9e3256f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efc10a-1a4a-4ae2-8baf-e503231630a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25721c18-c256-4893-9334-97db8f1d2680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b393de3-e580-48e9-9a34-1d0f3acec59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1fe1c-ecbf-4455-8ed4-064541a06e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88237de-7829-47ae-8b07-1d8e192957c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7253a5-ecec-4937-9830-bd593f1f8537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b1ee9-ac20-476f-b56c-f59db3d646b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e2553-175b-4912-a06e-2d11ac21cd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
